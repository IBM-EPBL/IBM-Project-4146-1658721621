# -*- coding: utf-8 -*-
"""Vehicle Performance Analyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nJLdIz1sl4xxmFKCpdEiSGoNbCu_xq4P

# Importing Libraries
"""

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

"""# Importing Dataset"""

import os, types
import pandas as pd
from botocore.client import Config
import ibm_boto3

def __iter__(self): return 0

# @hidden_cell
# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.
# You might want to remove those credentials before you share the notebook.
cos_client = ibm_boto3.client(service_name='s3',
    ibm_api_key_id='qExQR3pYSJKQYDf-ENb2R_ZjfIxj5xM980bx-ep2r-eW',
    ibm_auth_endpoint="https://iam.cloud.ibm.com/oidc/token",
    config=Config(signature_version='oauth'),
    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')

bucket = 'machinelearningbasedvehicleperfor-donotdelete-pr-rngtoskacnge4j'
object_key = 'car performance (1).csv'

body = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']
# add missing __iter__ method, so pandas accepts body as file-like object
if not hasattr(body, "__iter__"): body.__iter__ = types.MethodType( __iter__, body )

dataset= pd.read_csv(body)
dataset.head()

"""# Finding missing data"""

dataset.isnull().any()

"""There are no null characters in the columns but there is a special character '?' in the 'horsepower' column. So we we replaced '?' with nan and replaced nan values with mean of the column."""

dataset['horsepower']=dataset['horsepower'].replace('?',np.nan)

dataset['horsepower'].isnull().sum()

dataset['horsepower']=dataset['horsepower'].astype('float64')

dataset['horsepower'].fillna((dataset['horsepower'].mean()),inplace=True)

dataset.isnull().any()

dataset.info() #Pandas dataframe.info() function is used to get a quick overview of the dataset.

dataset.describe() #Pandas describe() is used to view some basic statistical details of a data frame or a series of numeric values.

"""There is no use with car name attribute so drop it"""

dataset=dataset.drop('car name',axis=1) #dropping the unwanted column.

corr_table=dataset.corr()#Pandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. 
corr_table

"""# Data Visualizations

Heatmap : which represents correlation between attributes
"""

sns.heatmap(dataset.corr(),annot=True,linecolor ='black', linewidths = 1)#Heatmap is a way to show some sort of matrix plot,annot is used for correlation.
fig=plt.gcf()
fig.set_size_inches(8,8)

"""Visualizations of each attributes w.r.t rest of all attributes"""

sns.pairplot(dataset,diag_kind='kde') #pairplot represents pairwise relation across the entire dataframe.
plt.show()

"""Regression plots(regplot()) creates a regression line between 2 parameters and helps to visualize their linear relationships."""

sns.regplot(x="cylinders", y="mpg", data=dataset)

sns.regplot(x="displacement", y="mpg", data=dataset)

sns.regplot(x="horsepower", y="mpg", data=dataset)

sns.regplot(x="weight", y="mpg", data=dataset)

sns.regplot(x="acceleration", y="mpg", data=dataset)

sns.regplot(x="model year", y="mpg", data=dataset)

sns.regplot(x="origin", y="mpg", data=dataset)

sns.set(style="whitegrid")
sns.boxplot(x=dataset["mpg"])

"""Finding quartiles for mgp

# The P-value is the probability value that the correlation between these two variables is statistically significant. 
Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between 
the variables is significant.

By convention, when the
<ul>
    <li>p-value is $<$ 0.001: we say there is strong evidence that the correlation is significant.</li>
    <li>the p-value is $<$ 0.05: there is moderate evidence that the correlation is significant.</li>
    <li>the p-value is $<$ 0.1: there is weak evidence that the correlation is significant.</li>
    <li>the p-value is $>$ 0.1: there is no evidence that the correlation is significant.</li>
</ul>
"""

from scipy import stats

"""<h3>Cylinders vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'Cylinders' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['cylinders'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>

<p>Since the p-value is $<$ 0.001, the correlation between cylinders and mpg is statistically significant, and the coefficient of ~ -0.775 shows that the relationship is negative and moderately strong.

<h3>Displacement vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'Displacement' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['displacement'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>

<p>Since the p-value is $<$ 0.1, the correlation between displacement and mpg is statistically significant, and the linear negative relationship is quite strong (~-0.809, close to -1)</p>

<h3>Horsepower vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'horsepower' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['horsepower'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>

<p>Since the p-value is $<$ 0.001, the correlation between horsepower and mpg is statistically significant, and the coefficient of ~ -0.771 shows that the relationship is negative and moderately strong.

<h3>Weght vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'weight' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['weight'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>

<p>Since the p-value is $<$ 0.001, the correlation between weight and mpg is statistically significant, and the linear negative relationship is quite strong (~-0.831, close to -1)</p>

<h3>Acceleration vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'Acceleration' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['acceleration'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>
<p>Since the p-value is $>$ 0.1, the correlation between acceleration and mpg is statistically significant, but the linear relationship is weak (~0.420).</p>

<h3>Model year vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'Model year' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['model year'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>
<p>Since the p-value is $<$ 0.001, the correlation between model year and mpg is statistically significant, but the linear relationship is only moderate (~0.579).</p>

<h3>Origin vs mpg</h3>

Let's calculate the Pearson Correlation Coefficient and P-value of 'Origin' and 'mpg'.
"""

pearson_coef, p_value = stats.pearsonr(dataset['origin'], dataset['mpg'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)

"""<h5>Conclusion:</h5>
<p>Since the p-value is $<$ 0.001, the correlation between origin and mpg is statistically significant, but the linear relationship is only moderate (~0.563).</p>

<b>Ordinary Least Squares</b>  Statistics
"""

test=smf.ols('mpg~cylinders+displacement+horsepower+weight+acceleration+origin',dataset).fit()
test.summary()

"""Inference as in the above summary the p value of the accelaration is maximum(i.e 0.972) so we can remove the acc variable from the dataset

# Seperating into Dependent and Independent variables

<b>Independent variables</b>
"""

x=dataset[['cylinders','displacement','horsepower','weight','model year','origin']].values
x

"""<b>Dependent variables</b>"""

y=dataset.iloc[:,0:1].values
y

"""# Splitting into train and test data."""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=0)

"""we are splitting as 90% train data and 10% test data

# decision tree regressor
"""

from sklearn.tree import DecisionTreeRegressor
dt=DecisionTreeRegressor(random_state=0,criterion="mae")
dt.fit(x_train,y_train)

import pickle
pickle.dump(dt,open('decision_model.pkl','wb'))

y_pred=dt.predict(x_test)
y_pred

y_test

pip install pydotplus

import os
import six
import sys
sys.modules['sklearn.externals.six'] = six
os.environ['PATH'] = os.environ['PATH']+';'+os.environ['CONDA_PREFIX']+r"\Library\bin\graphviz"
from sklearn.externals.six import StringIO  
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(dt, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

ax1 = sns.distplot(dataset['mpg'], hist=False, color="r", label="Actual Value")
sns.distplot(y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)


plt.title('Actual vs Fitted Values for mpg')
plt.xlabel('mpg')
plt.ylabel('Proportion of Cars')
 
plt.show()
plt.close()

"""We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.

<b>R-squared</b>
<p>R-squared is a statistical measure of how close the data are to the fitted regression line. 
It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.</p>


R-squared = Explained variation / Total variation

<b>Mean Squared Error (MSE)</b>

<p>The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value (y) and the estimated value (ŷ).</p>
"""

from sklearn.metrics import r2_score,mean_squared_error

r2_score(y_test,y_pred)

mean_squared_error(y_test,y_pred)

np.sqrt(mean_squared_error(y_test,y_pred))

"""# random forest regressor"""

from sklearn.ensemble import RandomForestRegressor

rf= RandomForestRegressor(n_estimators=10,random_state=0,criterion='mae')
rf.fit(x_train,y_train)

y_pred2=rf.predict(x_test)
y_pred2

ax1 = sns.distplot(dataset['mpg'], hist=False, color="r", label="Actual Value")
sns.distplot(y_pred2, hist=False, color="b", label="Fitted Values" , ax=ax1)


plt.title('Actual vs Fitted Values for mpg')
plt.xlabel('mpg')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()

"""We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement."""

from sklearn.metrics import r2_score,mean_squared_error

r2_score(y_test,y_pred2)

mean_squared_error(y_test,y_pred2)

np.sqrt(mean_squared_error(y_test,y_pred2))

"""# linear regression"""

from sklearn.linear_model import LinearRegression
mr=LinearRegression()
mr.fit(x_train,y_train)

y_pred3=mr.predict(x_test)
y_pred3

ax1 = sns.distplot(dataset['mpg'], hist=False, color="r", label="Actual Value")
sns.distplot(y_pred3, hist=False, color="b", label="Fitted Values" , ax=ax1)


plt.title('Actual vs Fitted Values for mpg')
plt.xlabel('mpg')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()

"""We can see that the fitted values are not as close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement."""

from sklearn.metrics import r2_score,mean_squared_error
r2_score(y_test,y_pred3)

mean_squared_error(y_test,y_pred3)

np.sqrt(mean_squared_error(y_test,y_pred3))

"""<b>Conclusion:</b>
<p>When comparing models, the model with the higher R-squared value is a better fit for the data.</p>
<p>When comparing models, the model with the smallest MSE value is a better fit for the data.</p>

Comparing these three models, we conclude that the DecisionTree model is the best model to be able to predict mpg from our dataset. 

"""

!pip install -U ibm-watson-machine-learning

from ibm_watson_machine_learning import APIClient
import json
import numpy as np

wml_credentials={"apikey":"DEpPsc6_-XtpuOrjGq_w9-kubPIKtDx2hgucs4D1YPLz","url":"https://us-south.ml.cloud.ibm.com"}

wml_client=APIClient(wml_credentials)

wml_client.spaces.list()

SPACE_ID="751dd703-7418-422d-86bb-020683b1c79c"

wml_client.set.default_space(SPACE_ID)

wml_client.software_specifications.list()

import sklearn 
sklearn.__version__

MODEL_NAME='ML_model'
DEPLOYMENT_NAME='Demo_deploy'
DEMO_MODEL=dt

software_spec_uid=wml_client.software_specifications.get_id_by_name('runtime-22.1-py3.9')

model_props={wml_client.repository.ModelMetaNames.NAME:MODEL_NAME,wml_client.repository.ModelMetaNames.TYPE:'scikit-learn_1.0',wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID:software_spec_uid}

model_details=wml_client.repository.store_model(model=DEMO_MODEL,meta_props=model_props,training_data=x_train,training_target=y_train)

model_details

model_id=wml_client.repository.get_model_id(model_details)

model_id

deployment_props={wml_client.deployments.ConfigurationMetaNames.NAME:DEPLOYMENT_NAME,wml_client.deployments.ConfigurationMetaNames.ONLINE:{}}

deployment=wml_client.deployments.create(artifact_uid=model_id,meta_props=delpoyment_props)